{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 11:42:50.124706: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ayesharahman1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ayesharahman1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024-02-09 11:43:03.780590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 10s 150ms/step - loss: 0.4021 - accuracy: 0.8853 - val_loss: 0.0848 - val_accuracy: 0.9837\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 8s 154ms/step - loss: 0.0264 - accuracy: 0.9950 - val_loss: 0.0111 - val_accuracy: 0.9975\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 6.0505e-04 - accuracy: 1.0000 - val_loss: 8.8168e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 3.5788e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 3.9956e-04 - accuracy: 1.0000 - val_loss: 8.7867e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 2.1402e-04 - accuracy: 1.0000 - val_loss: 8.5708e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 1.7062e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 0.9987\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 1.3867e-04 - accuracy: 1.0000 - val_loss: 8.3926e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 1.3189e-04 - accuracy: 1.0000 - val_loss: 4.3270e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 10s 199ms/step - loss: 1.2932e-04 - accuracy: 1.0000 - val_loss: 8.6118e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 7.2308e-05 - accuracy: 1.0000 - val_loss: 8.6763e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 11s 214ms/step - loss: 7.2138e-05 - accuracy: 1.0000 - val_loss: 5.7351e-04 - val_accuracy: 1.0000\n",
      "32/32 [==============================] - 3s 67ms/step\n",
      "Accuracy: 0.9990\n",
      "Precision: 1.0000\n",
      "Recall: 0.9980\n",
      "F1-Score: 0.9990\n",
      "INFO:tensorflow:Assets written to: ram://5cecddf3-975a-4aa3-86b1-3b7976093637/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "with open(\"synthetic_dataset_with_notes.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['processed_note'] = df['note'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['processed_note'])\n",
    "sequences = tokenizer.texts_to_sequences(df['processed_note'])\n",
    "X = pad_sequences(sequences, maxlen=100)\n",
    "y = df['label'].values\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=20, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(X_test)\n",
    "predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions, average='binary')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'autism_classifier.joblib')\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(tokenizer, 'tfidf_vectorizer.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2371310abb41f4596ec457ca1e0992df68ce942553f6c0e8ef424c6dc23731f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('flask': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
